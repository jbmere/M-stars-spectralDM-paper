
In the following, we will summarise the results obtained for the IRTF
data set. We deal with the different physical paramenters in separate
Sections. We start by reporting the Root Mean/Median Square Errors
(RMSE/RMDSE) with respect to the parameters gathered from the
literature by \cite{cesetti} and included in their Table 3.

\subsubsection{Effective temperature models}

Table \ref{tab:model_TSD} summarises the RMSE/RMDSE for the complete
set of models: the minimum $\chi^2$ estimate based on the full
spectrum ($\chi^2$), the projection pursuit regression based on the
ICA components (PPR-ICA) and models trained on the spectral features
proposed by the GA (GA-RF, GA-GBM, GA-SVR, GA-NNET, GA-MARS,
GA-KPLS, GA-RR). For each model, we report the RMSE/RMDSE obtained for
several noise levels of the training sets.  SNR=$\infty$ corresponds
to noiseless spectra. In the GA- cases, the model is trained with
  the spectral features found by the Genetic Algorithms when applied
  to BT-Settl spectra of the corresponding SNR.

{\bf Make sure we always have Rule-Regression models everywhere or
  discuss why not.}
  
Table \ref{tab:model_TSD} shows that the performance of classifiers
based on the full spectrum (or in a compressed version in the form of
ICA components) and the best classifier based on features derived from
limited spectral bands is equivalent. The bartlett test shows that the
variances are homogeneous with a Bartlett\textquoteright s K-squared
of 8.5 with 2 degrees of freedom and a p-value of0.01426. The
Flinger-Killen test shows that homokedascity is verified at the
p=0.005886 level. Finally, the F-ANOVA test clearly shows that there
is no significant difference between models. Thus, we conclude that
the quality of features from the two approaches are equivalent in
predictive performance.  The difference between the performances of
the best classifier ($GA-KNN$; best on average over SNR), the minimum
$\chi^2$ classifier, and the $PPR-ICA$ classifiers are not
statistically significant. The bartlett test shows that the variances
are homogeneous with a Bartlett\textquoteright s K-squared of 8.5 with
2 degrees of freedom and a p-value of0.01426. The Flinger-Killen test
shows that homokedascity is verified at the p=0.005886 level. Finally,
the F-ANOVA test clearly shows that there is no significant difference
between models. Thus, we conclude that the quality of features from
the two approaches are equivalent in predictive performance.  In any
case, it is evident that the RMSE is significantly above the grid
spacing in temperature. We interpret the small differences as an
indication that there is as much information spread over the entire
spectrum shape as can be distilled from a few spectral bands.

The comparison with the effective temperatures compiled by
\cite{cesetti} shows however some significant differences across
models when evaluated not by the RMSE/RMDSE, but by the average bias
(see Table \ref{tab:model_Tbias}). 

In general, all classifiers tend to predict lower effective
temperatures than those in the literature except in the noiseless
scenario. The models trained with noiseless spectra tend to
overestimate $T_{\rm eff}$, suggesting that the optimal SNR is between
SNR=50 and $\infty$. The minimum-$\chi^2$ approach and the GA-KNN
model systematically underestimate $T_{\rm eff}$ for all SNR
regimes. This shared behaviour is not surprising since minimum
$\chi^2$ is a single nearest neighbour method applied in the space of
the entire spectrum as opposed to the space selected features.

We have found in previous studies that, at least for input spaces
constructed from ICA compressions of the spectra, it is not necessary
to adapt the training set SNR to match exactly that of the prediction
set. On the contrary, we find that two regimes are sufficient to
obtain acceptable results. The two regimes are separated at
SNR=10. The model trained with SNR=50 spectra gives close to optimal
results for spectra with SNRs above 10, while below that limit the
same situation holds for the model trained with SNR=10 spectra. {\bf
  Cite paper by Ana.}

Figure~\ref{fig:irtf-teff} shows the correlation between the $T_{\rm
eff}$ estimates of the best (in the RMDSE sense) regression models and
the effective temperatures in Table 3 of \cite{cesetti}. 

%\begin {figure}
% \centering
%  \includegraphics[width=11cm]{figs/irtf-teff.pdf}
%  \caption{}
% \label{fig:irtf-teff}
%\end {figure}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comparison with Teffs from spectral types.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We then compare the predicted effective temperatures with the spectral
types listed in the IRTF spectral library in order to increase the
size of the validation sample beyond the 57 cases with estimated
temperatures in Table 3 of \cite{cesetti}. We converted the spectral
types into effective temperatures using the calibration of
\cite{2009ApJ...702..154S}. Both the RMSE and RMDSE were used to
evaluate the prediction accuracy (see Table~\ref{tab:model_Tvar}).

{\bf Faltan las tablas y figuras.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TBD: Comparison with temperatures estimated with Cesetti features
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We have trained the same non linear regression models discussed above
using the features suggested by \cite{cesetti}. The performace of the
models based on these features are included in Table
\ref{tab:tab_CS_Model}.


{\bf How do you explain that the best SNR=10 model has the poorest
  performances for SNR=50 or $\infty$?}

From the comparison of Tables \ref{tab:tab_CS_Model} and
\ref{tab:model_TSD} we can draw the following conclusions:

\begin{itemize}
\item the RMSE for SNR=10 and 50 is equivalent for the regression
  models trained on GA features and those recommended in
  \cite{cesetti};
  \item however, the RMDSE is significantly higher in the case of the
    latter features for all SNR values.
    \item in the unrealistic case of noiseless spectra, the features
      proposed by \cite{cesetti} produce RMSE and RMDSE significantly
      worse than the GA features.
\end{itemize}

As a summary, we believe that the features found by the GA are to be
prefered to the ones proposed by \cite{cesetti}.

\subsubsection{Surface gravity models}

For the validation of our models, we only have 10 literature values of
the surface gravity available in Table 3 of
\cite{cesetti}. Unfortunately, this is too small a number to draw
significant conclusions on the comparison of methodologies from
external data. Hence, we are left only with plausibility arguments for
the selection of models. In this Section we will use $\log(T_{\rm
  eff})--\log(g)$ diagram comparisons to select the most plausible
model results. An important difference with respect to the models
discussed above is that we use the $T_{\rm eff}$ estimated in the
previous stage as input of our models. {\bf do we have some hint
  whether this was beneficial, neutral or detrimental?}

Table~\ref{tab:models_G_rmse} shows the RMSE and RMDSE of the
$\log(g)$ regression models for the same SNR regimes discussed for the
estimation of $T_{\rm eff}$.

Again, as in the case of the effective temperatures, the differences
between the various models as measured by the RMSE or RMDSE are not
statistically significant.  This is not surprising given the
extraordinarily small sample of gravity measurements gathered from the
literature and used as reference for the computation of errors.
However, we can evaluate the models according to plausibility
arguments relative to the distribution of the model predictions in
$T_{\rm eff}$--$\log(g)$ diagrams.  Figure~\ref{fig:lt_lg_ga} shows
this distribution for four models selected based on these plausibility
criteria: GA-RR, GA-PLS, GA-KNN (the three of them for SNR=50), and
PPR-ICA (clockwise, starting at the top left corner).

\begin{figure*}
 \begin{center}
   \includegraphics[width=\textwidth]{figs/ordieres-fig4.pdf}
 \caption{$\log(T_{eff})$--$\log(g)$ diagrams produced by the GA-KNN
   (SNR=$\infty$) effective temperatures and gravities derived with
   the GA-RR (SNR=50), GA-PLS (SNR=50), GA-NNR (SNR=50), and $\chi^2$ models (clockwise, starting from
   the top left plot).}
 \label{fig:lt_lg_ga}
 \end{center}
\end{figure*}

{\bf Is $\chi^2$ much worse now for the weak parameter logg? I guess
  no. This needs be discussed}

{\bf Discuss these plots in the case of Cesetti features.}

\subsubsection{Metallicity models} 
\label{sect:irtf-met}
Finally, the same machine learning models are trained to infer the
metallicity, again considering the effective temperature as an input
feature as in the $\log(g)$ regression
models. Table~\ref{tab:Met-irtf-litCes} shows the RMSE and RMDSE
obtained for each regression model for the only seven M-type stars in
Table 3 of \cite{cesetti}. The minimum values are consistently obtained 
with the minimum $\chi^2$, $PPR-ICA$ and $GA-KNN$. The differences are 
only marginal, but we see that even at these intermediate resolutions 
the reduction of dimensionality (either with ICA or GA) produces an 
improvement in the predictions. This is even more evident if we 
compare our predictions with more recent metallicity estimates not 
included in \cite{cesetti}. We have gathered estimates for stars in 
both the IRTF collection and a series of recent metallicity catalogs 
by \cite{RA2012}, \cite{NevesIII}, \cite{Newton2014}, \cite{Gaidos2015}, 
and \cite{Mann2015}. All of the aforementioned references provide us 
with estimates of the iron abundance ratio [Fe/H] except \cite{RA2012} that 
provides both the overall metallicity [M/H]
and the [Fe/H] ratio. Our estimates, coming from the BT-Settl library, are 
for the [M/H] ratios, so some offset could be expected from the 
different nature of the quantities compared. Hence, when comparing our 
estimates with those from the literature, we compute the RMSE or RMDSE 
after subtracting any difference in the mean. It turns out that, after 
correcting for biases, $PPR-ICA$ trained with SNR=10 examples 
yields the lowest RMSE/RMDSE. Figure \ref{MIRTF_ICA_10} 
represents the estimates of [M/H] obtained from the $PPR-ICA$ based 
regressor, as a function of the values taken from these references for 
the sources in common. The black empty circles represent values from \cite{cesetti}
; orange filled circles, values from \cite{NevesIII};  green filled 
squares, values that the Vizier catalog entry for Table 8 of 
\cite{NevesIII} links to \cite{Jao}, although we find no evidence that \cite{Jao} 
contains estimates of metallicities; cyan and blue filled squares, the values 
of [M/H] and [Fe/H] respectively in \cite{RA2012}; red filled squares, values 
from \cite{Mann2015}; yellow filled squares,  values from \cite{Newton2014}; and, 
finally, black filles squares, values from \cite{Gaidos2015}.

%
% Metalicidad teórica desde Cesseti para las IRTF
%

{\bf Compare the 7 or 6 values available. Discuss.  $\chi^2$ is
  the most popular method by far. We compare predictions of machine
  learning methods with minimum chi-squared. We first do histogram
  plots. Then, the same logTeff-logg plots as above but with
  metallicity coded in colour.}

 \begin {figure}
  \centering
   \includegraphics[width=0.45\textwidth]{figs/irtf-figs/M-ICA10.pdf}
   \caption{Comparison between metallicity estimates from the
     literature and predictions from the PPR-ICA (SNR=10) model. 
     Black empty circles represent values from \cite{cesetti}
     ; orange filled circles, values from \cite{NevesIII};  green filled 
     squares, values that the Vizier catalog entry for Table 8 of 
     \cite{NevesIII} links to \cite{Jao}, although we find no evidence that \cite{Jao} 
     contains estimates of metallicities; cyan and blue filled squares, the values 
     of [M/H] and [Fe/H] respectively in \cite{RA2012}; red filled squares, values 
     from \cite{Mann2015}; yellow filled squares,  values from \cite{Newton2014}; and, 
     finally, black filles squares, values from \cite{Gaidos2015}.}
  \label{MIRTF_ICA_10}
 \end {figure}

 {\bf Include table as annex with metallicities from the literature?}
